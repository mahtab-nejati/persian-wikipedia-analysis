{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from functools import reduce\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import words\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import nltk\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the sparkcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"10g\"\n",
    "# sc.stop()\n",
    "spark = SparkSession.builder.appName(\"Foo\").config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Loading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "711032"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.json(\"text/*\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMP!!!! REMOVE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.json(\"text/AB/*\", multiLine=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id='238077', text='عبدالحمید ملک\\u200cالکلامی\\n\\nعبدالحمید ملک الکلامی معروف به امیرالکتاب (۱۲۶۲–۱۳۲۸ ه\\u200d. ش) یکی از خوشنویسان ایران است. وی به نستعلیق، ثلث، نسخ، شکسته\\u200cنستعلیق مسلط بود. او خطاطی مبتکر و فرزند میرزا عبدالمجید مجدی سقزی معروف بهمیرزا مجدالدین ملک الکلام کردستانی است. او در سال ۱۲۶۲ه\\u200d. ش در سنندج کردستان چشم به جهان گشود و در مهرماه ۱۳۲۸ه\\u200d. ش در سن شصت و چهار سالگی درگذشته و در تهران به خاک سپرده شده\\u200cاست.\\n\\nاو در انواع خطوط نسخ، ثلث، رقاع، نستعلیق و شکسته از استادان مسلم بود. این هنر مند خطوط طغرایی را بسیار زیبا می\\u200cنوشت و در ساختن مرکب و آهار و مهره کردن کاغذ استاد بود. افزون بر خط، او در نقاشی آب\\u200cرنگ نیز دست داشت. عبدالحمید ملک الکلامی به نقاری و حکاکی مسلط بود و بعضی سرسکه\\u200cهای ضرابخانه دولتی آن زمان را او تهیه کرده\\u200cاست.\\n\\nقریحه شاعری را از پدر به ارث برده بود و «شرقی» تخلص می\\u200cکرد. کتیبه\\u200cهای آرامگاه حافظ در شیراز و کتیبه\\u200cهای حجاری شده موزه ایران باستان به خط ثلث اوست.\\n', title='عبدالحمید ملک\\u200cالکلامی', url='https://fa.wikipedia.org/wiki?curid=238077')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to RDD & removing xml tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### index 0 => ID\n",
    "###### index 1 => URL\n",
    "###### index 2 => Title\n",
    "###### index 3 => Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('238077',\n",
       " 'https://fa.wikipedia.org/wiki?curid=238077',\n",
       " 'عبدالحمید ملک\\u200cالکلامی',\n",
       " 'عبدالحمید ملک\\u200cالکلامی عبدالحمید ملک الکلامی معروف به امیرالکتاب (۱۲۶۲–۱۳۲۸ ه\\u200d. ش) یکی از خوشنویسان ایران است. وی به نستعلیق، ثلث، نسخ، شکسته\\u200cنستعلیق مسلط بود. او خطاطی مبتکر و فرزند میرزا عبدالمجید مجدی سقزی معروف بهمیرزا مجدالدین ملک الکلام کردستانی است. او در سال ۱۲۶۲ه\\u200d. ش در سنندج کردستان چشم به جهان گشود و در مهرماه ۱۳۲۸ه\\u200d. ش در سن شصت و چهار سالگی درگذشته و در تهران به خاک سپرده شده\\u200cاست. او در انواع خطوط نسخ، ثلث، رقاع، نستعلیق و شکسته از استادان مسلم بود. این هنر مند خطوط طغرایی را بسیار زیبا می\\u200cنوشت و در ساختن مرکب و آهار و مهره کردن کاغذ استاد بود. افزون بر خط، او در نقاشی آب\\u200cرنگ نیز دست داشت. عبدالحمید ملک الکلامی به نقاری و حکاکی مسلط بود و بعضی سرسکه\\u200cهای ضرابخانه دولتی آن زمان را او تهیه کرده\\u200cاست. قریحه شاعری را از پدر به ارث برده بود و «شرقی» تخلص می\\u200cکرد. کتیبه\\u200cهای آرامگاه حافظ در شیراز و کتیبه\\u200cهای حجاری شده موزه ایران باستان به خط ثلث اوست.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def textCleanUp(text):\n",
    "    clean1 = re.compile(r'[\\n\\r\\t]')\n",
    "    clean2 = re.compile('<.*?>')\n",
    "    clean3 = re.compile(' +')\n",
    "    newT = re.sub(clean3,' ', re.sub(clean2, '',re.sub(clean1, ' ', text))).strip()\n",
    "    return newT\n",
    "    \n",
    "\n",
    "def getRDD(row):\n",
    "    return (row['id'], row['url'], textCleanUp(row['title']), textCleanUp(row['text']))\n",
    "\n",
    "\n",
    "textual = df.rdd.map(lambda row: getRDD(row))\n",
    "textual.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: One-hundred most frequent words\n",
    "## 3: Least frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing into words & counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ملک\\u200cالکلامی', 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = textual.flatMap(lambda row: nltk.word_tokenize(str(row[2])+' '+str(row[3])))\n",
    "\n",
    "to_count = words.map(lambda word: (word,1))\n",
    "counted = to_count.reduceByKey(lambda a,b: a+b)\n",
    "counted.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20326"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countedDF = counted.toDF(['word','count'])\n",
    "countedDF = countedDF.sort('count',ascending=False)\n",
    "countedDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one hundred most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostFreq = countedDF.head(100)\n",
    "mostFreq = [row.word for row in mostFreq]\n",
    "len(mostFreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## least frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19336"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leastFreq = countedDF.filter('count < 20')\n",
    "leastFreq = leastFreq.select('word').rdd.map(lambda row: row[0]).collect()\n",
    "len(leastFreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Remove all least frequent and one hundred most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = mostFreq + leastFreq\n",
    "def remove_words(text,removee):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    result = [word for word in words if word not in removee]\n",
    "    return ' '.join(result)\n",
    "\n",
    "newText = textual.map(lambda row: (row[0],row[1],remove_words(row[2],to_remove),remove_words(row[3],to_remove)))\n",
    "# newText.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "890"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counted.count() - len(to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: One-hundred most frequent trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('سالگی', 'تهران', 'خاک'), 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = newText.flatMap(\n",
    "    lambda row: list(ngrams(nltk.word_tokenize(row[2]),3))+list(ngrams(nltk.word_tokenize(row[3]),3)))\n",
    "to_count = trigrams.map(lambda row: (row,1))\n",
    "counted = to_count.reduceByKey(lambda a,b: a+b)\n",
    "counted.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[trigram: struct<_1:string,_2:string,_3:string>, count: bigint]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countedDF = counted.toDF(['trigram','count'])\n",
    "countedDF = countedDF.sort('count',ascending=False)\n",
    "countedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostFreq = countedDF.head(100)\n",
    "mostFreq = [(row.trigram._1,row.trigram._2,row.trigram._3) for row in mostFreq]\n",
    "len(mostFreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: One-hundred most frequent English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "def extract_eng_words(word_list,vocab):\n",
    "    result = [word for word in word_list if word.lower() in vocab]\n",
    "    return result\n",
    "\n",
    "eng_words = newText.flatMap(\n",
    "    lambda row: \n",
    "    extract_eng_words(nltk.word_tokenize(row[2]),eng_vocab)+extract_eng_words(nltk.word_tokenize(row[2]),eng_vocab))\n",
    "to_count = eng_words.map(lambda row: (row,1))\n",
    "counted = to_count.reduceByKey(lambda a,b: a+b)\n",
    "counted_list = counted.collect()\n",
    "counted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(counted_list) > 0 :\n",
    "    countedDF = counted.toDF(['word','count'])\n",
    "    countedDF = countedDF.sort('count',ascending=False)\n",
    "    countedDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(counted_list) > 0 :\n",
    "    mostFreq = countedDF.head(100)\n",
    "    mostFreq = [row.word for row in mostFreq]\n",
    "    mostFreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Longest text URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://fa.wikipedia.org/wiki?curid=140680'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_length = newText.map(lambda row: ((row[0],row[1],row[2],row[3]),len(row[3])))\n",
    "text_length.max(lambda row: row[1])[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: My name is not that common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_name = newText.filter(lambda row : \n",
    "                         (('mahtab' in (row[2]+' '+row[3]).lower()) or ('مهتاب' in row[2]+' '+row[3])))\n",
    "my_name_list = my_name.collect()\n",
    "len(my_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6: Histogram of text length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to choose the best bin sizes, we need to work with the whole data.\n",
    "### Assuming the distribtion of the length of text to be normal, bin width needs to be minimal around the mean value of the data and scattered data can be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_list = text_length.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_x = [l[1] for l in length_list]\n",
    "len(length_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(length_x, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Creating TF-IDF dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = newText.map(lambda row: [row[0],row[1],row[2],str(row[2])+' '+str(row[3])])\n",
    "countDocs = text.count()\n",
    "\n",
    "def insertTFIDF(row):\n",
    "    res = []\n",
    "    words =  nltk.word_tokenize(row[-1])\n",
    "    return [[(row[0],row[1],row[2],len(words),word),1] for word in words]\n",
    "\n",
    "tfidf = newText.flatMap(insertTFIDF)\n",
    "tfidf = tfidf.reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "tfidf = tfidf.map(lambda row: [(row[0][0],row[0][1],row[0][2]),\n",
    "                               row[0][-1],float(row[1])/float(row[0][-2]),row[0][-2]])\n",
    "\n",
    "idfCompute = tfidf.map(lambda row: (row[1],1))\n",
    "idfCompute = idfCompute.reduceByKey(lambda a,b: float(a+b))\n",
    "# tfidf.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4473131088235682"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfOrder = list(set([i[0] for i in idf]))\n",
    "\n",
    "idfDict = dict(idf)\n",
    "idfDict['جهان']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('238077', 'https://fa.wikipedia.org/wiki?curid=238077', '', 'جهان'),\n",
       " 0.1566960504010713]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfComputed = tfidf.map(lambda row : [(row[0][0],row[0][1],row[0][2],row[1]),row[2]*idfDict[row[1]]])\n",
    "tfidfComputed.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17305"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def makeFinal(row):\n",
    "    result = [row]\n",
    "    for word in idfOrder:\n",
    "        result.append([(row[0][0],row[0][1],row[0][2],word),-1])\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "finalTFIDF = tfidfComputed.flatMap(makeFinal)\n",
    "finalTFIDF = finalTFIDF.reduceByKey(lambda a,b : max(a,b))\n",
    "finalTFIDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
